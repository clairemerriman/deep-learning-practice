{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "Since my [academic research](https://scholar.google.com/citations?user=PDlwn_MAAAAJ&hl=en&oi=sra) in [ergodic theory](https://en.wikipedia.org/wiki/Ergodic_theory) and [symbolic dynamics](https://en.wikipedia.org/wiki/Symbolic_dynamics) also uses Shannon entropy, I actually started with [Information Theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html). I have also added some more notes from [Shannon's original paper](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) and other theoretical background. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information theory and background from _Shannon_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x_i$ is an _event_ encoded with $n$ bits of information, and the probability of $x_i$ occuring is $p(x_i),$ then the  \n",
    "_self-information_ of $x_i$ is $$I(x_i)=-\\log_2(p(x_i)).$$\n",
    "\n",
    "In the context of encoding information, we are focused on bits of information ($0$ or $1$). Thus, $x_i \\in (0,1)^n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "Since my [academic research](https://scholar.google.com/citations?user=PDlwn_MAAAAJ&hl=en&oi=sra) in [ergodic theory](https://en.wikipedia.org/wiki/Ergodic_theory) and [symbolic dynamics](https://en.wikipedia.org/wiki/Symbolic_dynamics) uses Kolmogorov-Sinai entropy, based on Shannon entroy, I actually started with [Information Theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html). I have also added some more notes from [Shannon's original paper](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) and other theoretical background. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of encoding information, we are focused on bits of information ($0$ or $1$), so we use $\\log_2,$ but the same definition works using other bases. In particular, natural log is more common in theoretical mathematical work.\n",
    "\n",
    "If $x_i$ is an _event_ encoded with $n$ bits of information, and the probability of $x_i$ occuring is $p_i,$ then the  \n",
    "_self-information_ or _information content_ of $x_i$ is $$I(x_i)=-\\log_2 p_i=\\log_2 \\tfrac{1}{p_i}.$$ The self-information is also described as the _surprise_ at the event occuring. Note that $$\\lim_{p\\to 0^+}\\log_2 \\tfrac{1}{p}= 
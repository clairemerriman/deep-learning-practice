{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "There is some pretty terrible mixing of notations, and generally horrible notation, for indices in the mathematical sections (TeX code)--I was just checking that I understood what would happen for various calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch` tensors basics\n",
    "\n",
    "From Chapter 3 of the PyTorch book: This chapter focuses heavily on the syntax and typing of tensors.\n",
    "\n",
    "From [Dive into Deep Learning Chapter 2](https://d2l.ai/chapter_preliminaries/linear-algebra.html#tensors): This section discusses arithemtic with tensors, as well as some preformance and memory considerations.\n",
    "\n",
    "\n",
    "A _tensor_ is also a multidimensional array. A PyTorch tensor can preform very fast operations on the GPU, distribute calculations over multiple devices, and keep track of the graph of computations, as opposed to a NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# create a 1d tensor of size 3 with all 1s\n",
    "a = torch.ones(3) \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor whose points are the corners of a triangle\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry is its own tensor, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`points[0]` is the 1D tensor of size $2$ `tensor([4., 1.])`. \n",
    "\n",
    "We can see the shape and size of these tensors by using `points.shape` and `points[0].shape`. Both outputs have the form `torch.Size([...])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `arange` to generate entries and `reshape`  to specify the shape. We can also specify the data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `.zeros_` and a few other operations (which ones?) with a trailing underscore to update the definition of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.zero_()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate numbers 0 to 23\n",
    "# then create a 3d tensor with two 3x4 matrices\n",
    "X=torch.arange(24).reshape(2, 3, 4)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries of `X` should be (indexed from $0$)\n",
    "    $$X[0]=\\begin{bmatrix}\n",
    "        X_{000} &   X_{001} & X_{002} & X_{003} \\\\\n",
    "        X_{010} &   X_{011} & X_{012} & X_{013} \\\\\n",
    "        X_{020} &   X_{021} & X_{022} & X_{023}\n",
    "    \\end{bmatrix},\n",
    "    X[1]=\\begin{bmatrix}\n",
    "        X_{100} &   X_{101} & X_{102} & X_{103} \\\\\n",
    "        X_{110} &   X_{111} & X_{112} & X_{113} \\\\\n",
    "        X_{120} &   X_{121} & X_{122} & X_{123}\n",
    "    \\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out parts of the tensor using indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]),\n",
       " tensor([4, 5, 6, 7]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], X[1],X[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also preform operations with the tensors. If the sizes match, operations will be preformed entry-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12, 14, 16, 18],\n",
       "         [20, 22, 24, 26],\n",
       "         [28, 30, 32, 34]]),\n",
       " tensor([[  0,  13,  28,  45],\n",
       "         [ 64,  85, 108, 133],\n",
       "         [160, 189, 220, 253]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]+ X[1], X[0]* X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also transpose a matrix using `transpose` and preform \"standard\" matrix multiplication using `.mm` Dot products can be done with `torch.dot(x,y)` and a matrix times a vector can be done with `torch.mv(A,x)`, where `x` and `y` are vectors (1D tensors) with `torch.Size([n])` and `A` is a matrix (2D tensor) with `torch.Size([m,n]).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8],\n",
       "        [ 1,  5,  9],\n",
       "        [ 2,  6, 10],\n",
       "        [ 3,  7, 11]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(X[0],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8],\n",
       "        [ 1,  5,  9],\n",
       "        [ 2,  6, 10],\n",
       "        [ 3,  7, 11]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[224, 236, 248, 260],\n",
       "         [272, 287, 302, 317],\n",
       "         [320, 338, 356, 374],\n",
       "         [368, 389, 410, 431]]),\n",
       " tensor([[ 86, 302, 518],\n",
       "         [110, 390, 670],\n",
       "         [134, 478, 822]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(X[0].transpose(0,1),X[1]),torch.mm(X[1],X[0].transpose(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `(0,1)` in `transpose` are saying to switch the $0^{th}$ and $1^{st}$ dimensions (rows and columns). \n",
    "\n",
    "Let's see what happens when we transpose the 3d tensor.\n",
    "Switching the $0^{th}$ and $1^{st}$ dimension should produce a tensor with shape `([3,2,4])` whose entries correspond to \n",
    "    $$\\begin{bmatrix}\n",
    "        X_{000} &   X_{001} & X_{002} & X_{003} \\\\\n",
    "        X_{100} &   X_{101} & X_{102} & X_{013}\n",
    "    \\end{bmatrix} \n",
    "    = \\begin{bmatrix}\n",
    "        0   &   1   & 2 & 3 \\\\\n",
    "        12  &   13  & 14& 15\n",
    "    \\end{bmatrix},$$\n",
    "    $$\\begin{bmatrix}\n",
    "        X_{010} &   X_{011} & X_{012} & X_{013} \\\\\n",
    "        X_{110} &   X_{111} & X_{112} & X_{113} \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "        4   &   5   & 6 & 7 \\\\\n",
    "        16  &   17  & 18& 19\n",
    "    \\end{bmatrix},$$\n",
    "    $$\\begin{bmatrix}\n",
    "        X_{020} &   X_{021} & X_{022} & X_{023} \\\\\n",
    "        X_{120} &   X_{121} & X_{122} & X_{123} \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "        8   &   9   & 10 & 11 \\\\\n",
    "        20  &   21  & 22 & 23\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "So the $0^{th}$ row of each original matrix becomes a row in the new $0^{th}$ matrix, the $1^{st}$ (index from 0) row of each original matrix becomes a row in the new $1^{st}$ matrix, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[ 4,  5,  6,  7],\n",
       "         [16, 17, 18, 19]],\n",
       "\n",
       "        [[ 8,  9, 10, 11],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(X,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, switching the $0^{th}$ and the 2nd dimension should produce a tensor with shape `([4,3,2])` whose entries correspond to\n",
    "    $$\\begin{bmatrix}\n",
    "        X_{000} &   X_{100}  \\\\\n",
    "        X_{010} &   X_{110}  \\\\\n",
    "        X_{020} &   X_{120}  \\\\\n",
    "    \\end{bmatrix} \n",
    "    = \\begin{bmatrix}\n",
    "        0   &   12\\\\\n",
    "        4   &   16\\\\\n",
    "        8   &   20\n",
    "    \\end{bmatrix},\n",
    "    \\begin{bmatrix}\n",
    "        X_{001} &   X_{101}  \\\\\n",
    "        X_{011} &   X_{111}  \\\\\n",
    "        X_{021} &   X_{121}  \\\\\n",
    "    \\end{bmatrix} \n",
    "    = \\begin{bmatrix}\n",
    "        1   &   13\\\\\n",
    "        5   &   17\\\\\n",
    "        9   &   21\n",
    "    \\end{bmatrix},$$\n",
    "    $$\\begin{bmatrix}\n",
    "        X_{002} &   X_{102}  \\\\\n",
    "        X_{012} &   X_{112}  \\\\\n",
    "        X_{022} &   X_{122}  \\\\\n",
    "    \\end{bmatrix} \n",
    "    = \\begin{bmatrix}\n",
    "        2   &   14\\\\\n",
    "        6   &   18\\\\\n",
    "        10  &   22\n",
    "    \\end{bmatrix},\n",
    "    \\begin{bmatrix}\n",
    "        X_{003} &   X_{103}  \\\\\n",
    "        X_{013} &   X_{113}  \\\\\n",
    "        X_{023} &   X_{123}  \\\\\n",
    "    \\end{bmatrix} \n",
    "    = \\begin{bmatrix}\n",
    "        3   &   15\\\\\n",
    "        6   &   18\\\\\n",
    "        10  &   23\n",
    "    \\end{bmatrix},$$\n",
    "So the $0^{th}$ column of each original matrix becomes a column in the new $0^{th}$ matrix, the $1^{st}$ (index from $0$) column of each original matrix becomes a column in the new $1^{st}$ matrix, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, 12],\n",
       "         [ 4, 16],\n",
       "         [ 8, 20]],\n",
       "\n",
       "        [[ 1, 13],\n",
       "         [ 5, 17],\n",
       "         [ 9, 21]],\n",
       "\n",
       "        [[ 2, 14],\n",
       "         [ 6, 18],\n",
       "         [10, 22]],\n",
       "\n",
       "        [[ 3, 15],\n",
       "         [ 7, 19],\n",
       "         [11, 23]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(X,0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction and norms\n",
    "\n",
    "We can _reduce_ the size of a tensor by summing the elements or by summing along a particular axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), tensor(276))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tensor $A$ has $n$-dimensions, the index for an entry is $A_{i_0 i_1 \\dots i_{n-1}}$. Summing along `axis=j` produses an $n-1$-dimensional tensor $B$ whose entries are $\\displaystyle B_{i_0 i_1 \\dots i_{j-1} i_{j+1} i_{n-1}}=\\sum_{i_j} A_{i_0 i_1 \\dots i_{j-1} i_j i_{j+1} i_{n-1}}$. This reduces the $j^{th}$ axes to one entry, removing it from the shape of the output.\n",
    "\n",
    "For our 3D tensors example, summing along `axis=0` is the same as summing the individual matrices. That is, we reduce the 2D slice (matrices) axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12, 14, 16, 18],\n",
       "         [20, 22, 24, 26],\n",
       "         [28, 30, 32, 34]]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=0),X.sum(axis=0).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our 3D tensors example, summing along `axis=1` is the same as summing the _columns_ of the original tensor. That is, we are reducing the axis corresponding to rows, but we do this by collapsing the number of rows, ie $B_{ij}=X_{i0j}+X_{i1j}+X_{i2j}$, the sum of the $j^{th}$ column in the $i^{th}$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12, 15, 18, 21],\n",
       "         [48, 51, 54, 57]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=1),X.sum(axis=1).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for our 3D tensors example, summing along `axis=2` is the same as summing the _rows_ of the original tensor. That is, we are reducing the axis corresponding to columns, but we do this by collapsing the number of column, ie $B_{ij}=X_{ij0}+X_{ij1}+X_{ij2}+X_{ij3}$, the sum of the $j^{th}$ row in the $i^{th}$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6, 22, 38],\n",
       "         [54, 70, 86]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=2),X.sum(axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing using mean, two different ways. Note that we want to make sure to use floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5000), tensor(2.5000))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sum (across various axes) without reducing the number of axes. Using our 2D tensor as an example, the size of the axis will be reduced to $1$, but the axis will not be eliminated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the cumulative sum funtion along an axis which does not reduce the number of axes. That is, if a tensor $A$ has $n$-dimensions, the index for an entry is $A_{i_0 i_1 \\dots i_{n-1}}$. The cummulative sum along `axis=j` produses an $n$-dimensional tensor $B$ whose entries are $\\displaystyle B_{i_0 i_1 \\dots i_{j}=k \\dots i_{n-1}}=\\sum_{i_j=0}^k A_{i_0 i_1 \\dots i_{j-1} i_j i_{j+1} i_{n-1}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 5., 7.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a brief reminder to myself about how $\\ell_p$ norms work\n",
    "    \n",
    "$$|| \\vec{x} ||_p = \\left(\\sum_{i} |x_i|^p \\right)^{1/p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is not immediately clear why we are discussing $\\ell_p$ norms, as the only examples are $\\ell_1$ (sum of absolute values of entries) and $\\ell_2$ (Euclidean), and the corresponding norms for matrices. For example, an $m\\times n$ matrix has _Frobenius norm_ \n",
    "    $$ ||A||_F = \\sqrt{\\sum_i \\sum_j |x_{ij}|^2}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4162)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "torch.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can generalize to\n",
    "    $$ ||A||_F = \\sqrt{\\sum_i \\sum_j \\cdots \\sum_n |x_{ij\\dots n}|^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.4833)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`? Write your answer without implementing any code, then check your answer using code.\n",
    "\n",
    "Answer: `len(X)` should be $2$, the size of the first axis $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a tensor X of arbitrary shape, does len(X) always correspond to the length of a certain axis of X? What is that axis?\n",
    "\n",
    "Answer: Yes, the $0^{th}$ axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `A / A.sum(axis=1)` and see what happens. Can you analyze the results?\n",
    "\n",
    "Answer: The dimensions do not match, so this does not work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m6\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A / A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider three large matrices, say $A\\in\\R^{2^{10}}\\times \\R^{2^{16}}, B\\in\\R^{2^{16}}\\times \\R^{2^{5}}$, and \n",
    "$C\\in\\R^{2^{5}}\\times\\R^{2^{14}}$ initialized with Gaussian random variables. You want to compute the product $ABC$. Is there any difference in memory footprint and speed, depending on whether you compute $(AB)C$ or $A(BC)$. Why?\n",
    "\n",
    "Answer: In either case, $ABC$ is in $\\R^{2^{10}}\\times \\R^{2^{14}}$\n",
    "- $AB$ is in $\\R^{2^{10}}\\times \\R^{2^{5}},$ where each of the $2^{15}$ entries was found using a dot product, ie the sum of $2^{16}$ products. This is a total of $2^{31}$ products. Then $(AB)C$ is an additional $2^{24}$ dot products, each of which sums $2^{5}$ products.\n",
    "- $BC$ is in $\\R^{2^{16}}\\times \\R^{2^{14}},$ where each of the $2^{30}$ entries was found using a dot product, ie the sum of $2^{14}$ products. This is a total of $2^{44}$ products. Then $A(BC)$ is an additional $2^{24}$ dot products, each of which sums $2^{14}$ products.\n",
    "\n",
    "$(AB)C$ will be faster and use less storage, as it reduces the dimension to $\\R^{2^{10}}\\times \\R^{2^{5}}$ in the first calculation, while $A(BC)$ increase the dimension to $\\R^{2^{16}}\\times \\R^{2^{14}}$. This is supported by running the code for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2**10,2**16)\n",
    "B = torch.randn(2**16,2**5)\n",
    "C = torch.randn(2**5,2**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 2 µs, total: 8 µs\n",
      "Wall time: 24.8 µs\n"
     ]
    }
   ],
   "source": [
    "torch.mm(torch.mm(A,B),C)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "torch.mm(A,torch.mm(B,C))\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider three large matrices, say $A\\in\\R^{2^{10}}\\times \\R^{2^{16}}, B\\in\\R^{2^{16}}\\times \\R^{2^{5}}$, and \n",
    "$C\\in\\R^{2^{5}}\\times\\R^{2^{16}}$ initialized with Gaussian random variables. Is there any difference in speed depending on whether you compute $AB$ or $AC^\\perp$? Why? What changes if you initialize $C=B^\\perp$ without cloning memory? Why?\n",
    "\n",
    "Answer: For the first question, where $B$ and $C^\\perp$ have the same dimension, but are otherwise unrelated, $AB$ is faster than $C^\\perp,$ since there are fewer computations. After running the code a few times, it seems that _generally_ $AB$ takes less CPU time, but $AC^\\perp$ takes less wall time.\n",
    "\n",
    "For the second, the only possible calculation is $AC^\\perp,$ which seems to also be very similar in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2**10,2**16)\n",
    "B = torch.randn(2**16,2**5)\n",
    "C = torch.randn(2**5,2**16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "torch.mm(A,B)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 1e+03 ns, total: 17 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "torch.mm(A,C.transpose(0,1))\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = B.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 1e+03 ns, total: 20 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "torch.mm(A,C.transpose(0,1))\n",
    "%time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
